{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpha_vantage as av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#training and predicting pipelines\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "#initial model evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "#Random Forrest Training\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_today(df_rates):\n",
    "    # Create a new dictionary with the date and other columns as the keys and values, respectively\n",
    "    new_row = {'date': '2023-11-09',\n",
    "            '5yr_rate': 4.65,\n",
    "            '10yr_rate': 4.63,\n",
    "            '30yr_rate': 4.77}\n",
    "\n",
    "    # Pass a list of indices to the index argument of the pd.DataFrame() constructor\n",
    "    new_row_df = pd.DataFrame(new_row, index=[0])\n",
    "\n",
    "    # Convert the date column of the new_row_df DataFrame to type Timestamp\n",
    "    new_row_df['date'] = pd.to_datetime(new_row_df['date'])\n",
    "\n",
    "    # Convert the date column of the df_rates DataFrame to type Timestamp\n",
    "    df_rates['date'] = pd.to_datetime(df_rates['date'])\n",
    "\n",
    "    # Concatenate the new_row DataFrame with the df_rates DataFrame\n",
    "    df_rates = pd.concat([df_rates, new_row_df], ignore_index=True)\n",
    "\n",
    "    # Sort the df_rates DataFrame by the date column in descending order\n",
    "    df_rates = df_rates.sort_values(by=['date'], ascending=False)\n",
    "\n",
    "    return df_rates\n",
    "\n",
    "def preprocess():\n",
    "        api_key = 'ZWKEQ2PS4DJY66FW'\n",
    "        #'ALUI1VJSESQR07TD'\n",
    "        ts = TimeSeries(key=api_key)\n",
    "        symbols=['QQQ', 'VXX']\n",
    "\n",
    "        #date range for filtered_date\n",
    "        start = '2009-01-30'\n",
    "        end = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        #pull in daily prices for each symbol\n",
    "        for sym in symbols:\n",
    "                data, meta_data = ts.get_daily(symbol=sym, outputsize='full')\n",
    "\n",
    "                #filter data for the specified date range\n",
    "                filtered_data = {date: values['4. close'] for date, values in data.items() if start <= date <= end}\n",
    "\n",
    "                #convert filtered_data into a DF\n",
    "                sym_df = pd.DataFrame.from_dict(filtered_data, orient='index')\n",
    "                sym_df.index = pd.to_datetime(sym_df.index)\n",
    "\n",
    "                #rename the column + use the symbol as the column header\n",
    "                sym_df = sym_df.rename(columns={0: sym})\n",
    "\n",
    "                #concatenate the DF for the current symbol to the main DF\n",
    "                df = pd.concat([df, sym_df], axis=1)\n",
    "\n",
    "        #calculate moving averages\n",
    "        sma = [50, 100, 200]\n",
    "\n",
    "        df_sma = df.sort_index(ascending=True) #make ascending to calc SMAs\n",
    "\n",
    "        #at each sma value, calculate the corresponding sma for each ticker\n",
    "        for num in sma:\n",
    "                col_name = f'QQQ_SMA_{num}' #pass through column name for each sma\n",
    "                df_sma[col_name] = df_sma['QQQ'].rolling(window=num).mean()\n",
    "\n",
    "        sma_vxx = [2, 3]\n",
    "        for num in sma_vxx:\n",
    "                col_name = f'VXX_SMA_{num}' #pass through column name for each sma\n",
    "                df_sma[col_name] = df_sma['VXX'].rolling(window=num).mean()\n",
    "\n",
    "        df_sma = df_sma.sort_index(ascending=False)\n",
    "\n",
    "        #remove rows with NaN\n",
    "        df_sma_clean = df_sma.dropna()\n",
    "        df_sma_clean = df_sma_clean.reset_index()\n",
    "        df_sma_clean = df_sma_clean.rename(columns={'index': 'date'})\n",
    "        df_sma_clean['date'] = pd.to_datetime(df_sma_clean['date'])\n",
    "\n",
    "        url = 'https://www.alphavantage.co/query?function=TREASURY_YIELD&interval=daily&maturity=5year&apikey='+api_key\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "\n",
    "        # Create a list of DataFrames, one for each year\n",
    "        df_5rate = []\n",
    "\n",
    "        # Create a DataFrame for the year\n",
    "        df_5rate = pd.DataFrame()\n",
    "        for i in data['data']:\n",
    "                df_5rate = pd.concat([df_5rate, pd.DataFrame({'date': i['date']}, index=[i['date']])], ignore_index=False)\n",
    "                df_5rate.loc[i['date'], '5yr_rate'] = i['value']\n",
    "\n",
    "        # Set the date column to a datetime type\n",
    "        df_5rate['date'] = pd.to_datetime(df_5rate['date'])\n",
    "\n",
    "        url = 'https://www.alphavantage.co/query?function=TREASURY_YIELD&interval=daily&maturity=10year&apikey='+api_key\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "\n",
    "        # Create a list of DataFrames, one for each year\n",
    "        df_10rate = []\n",
    "\n",
    "        # Create a DataFrame for the year\n",
    "        df_10rate = pd.DataFrame()\n",
    "        for i in data['data']:\n",
    "                df_10rate = pd.concat([df_10rate, pd.DataFrame({'date': i['date']}, index=[i['date']])], ignore_index=False)\n",
    "                df_10rate.loc[i['date'], '10yr_rate'] = i['value']\n",
    "\n",
    "        # Set the date column to a datetime type\n",
    "        df_10rate['date'] = pd.to_datetime(df_10rate['date'])\n",
    "\n",
    "        url = 'https://www.alphavantage.co/query?function=TREASURY_YIELD&interval=daily&maturity=30year&apikey='+api_key\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "\n",
    "        # Create a list of DataFrames, one for each year\n",
    "        df_30rate = []\n",
    "\n",
    "        # Create a DataFrame for the year\n",
    "        df_30rate = pd.DataFrame()\n",
    "        for i in data['data']:\n",
    "                df_30rate = pd.concat([df_30rate, pd.DataFrame({'date': i['date']}, index=[i['date']])], ignore_index=False)\n",
    "                df_30rate.loc[i['date'], '30yr_rate'] = i['value']\n",
    "\n",
    "        # Set the date column to a datetime type\n",
    "        df_30rate['date'] = pd.to_datetime(df_30rate['date'])\n",
    "\n",
    "        df_rates = df_5rate\n",
    "        df_rates = df_rates.merge(df_10rate[['date', '10yr_rate']], on='date')\n",
    "        df_rates = df_rates.merge(df_30rate[['date', '30yr_rate']], on='date')\n",
    "\n",
    "        rates_backup = df_rates\n",
    "\n",
    "        df_rates = rate_today(df_rates)\n",
    "\n",
    "        symbol = 'QQQ' \n",
    "        #set up date column\n",
    "        url = 'https://www.alphavantage.co/query?function=RSI&symbol='+symbol+'&interval=daily&time_period=10&series_type=open&apikey='+api_key\n",
    "        rsi_resp = requests.get(url)\n",
    "        rsi = json.loads(rsi_resp.content)\n",
    "\n",
    "        rsi_data = rsi['Technical Analysis: RSI']\n",
    "\n",
    "        df_rsi = pd.DataFrame([(date, rsi['RSI']) for date, rsi in rsi_data.items()], columns=['date', 'RSI'])\n",
    "        df_rsi['date'] = pd.to_datetime(df_rsi['date'])\n",
    "\n",
    "        #calculate moving averages\n",
    "        rsi = [2, 3]\n",
    "\n",
    "        df_rsi = df_rsi.sort_index(ascending=False) #make ascending to calc aves\n",
    "\n",
    "        for num in rsi:\n",
    "                col_name = f'RSI_{num}' #pass through column name for each sma\n",
    "                df_rsi[col_name] = df_rsi['RSI'].rolling(window=num).mean()\n",
    "\n",
    "        df_rsi = df_rsi.sort_index(ascending=True) #make ascending to calc aves\n",
    "\n",
    "        df_continuous = df_sma_clean\n",
    "        df_continuous = df_continuous.merge(df_rates[['date', '5yr_rate', '10yr_rate', '30yr_rate']], on='date')\n",
    "        df_continuous = df_continuous.merge(df_rsi[['date', 'RSI', 'RSI_2', 'RSI_3']], on='date')\n",
    "\n",
    "        return df_continuous\n",
    "\n",
    "def traintest(days_predict, df_continuous):    \n",
    "    \n",
    "    df_cont = df_continuous.copy()\n",
    "    df_cont = df_cont.drop('date', axis=1)\n",
    "    df_cont = df_cont.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_continuous_scaled = scaler.fit_transform(df_cont)\n",
    "    df_continuous_scaled = pd.DataFrame(df_continuous_scaled, columns=df_cont.columns)\n",
    "\n",
    "    days_predict = days_predict\n",
    "\n",
    "    # Define a function to add a new column to the DataFrame\n",
    "    def add_qqq_greater_column(df):\n",
    "        df['QQQ_'+str(days_predict)] = df['QQQ'] > df['QQQ'].shift(-days_predict)\n",
    "        return df\n",
    "\n",
    "    # Apply the function to the DataFrame\n",
    "    df_continuous_scaled = add_qqq_greater_column(df_continuous_scaled.copy())\n",
    "\n",
    "    df_continuous_scaled['QQQ_'+str(days_predict)] = df_continuous_scaled['QQQ_'+str(days_predict)].map({True: 1, False: 0})\n",
    "\n",
    "    df_continuous_scaled = df_continuous_scaled.dropna()\n",
    "\n",
    "    counts = df_continuous_scaled['QQQ_'+str(days_predict)].value_counts()\n",
    "    pos = counts[1]\n",
    "    neg = counts[0]\n",
    "\n",
    "    resample_size = max(pos, neg)\n",
    "    resample_size\n",
    "\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    positive = df_continuous_scaled[df_continuous_scaled['QQQ_'+str(days_predict)]==1]\n",
    "    negative = df_continuous_scaled[df_continuous_scaled['QQQ_'+str(days_predict)]==0]\n",
    "\n",
    "    if len(positive) != resample_size:\n",
    "        positive_balanced = resample(positive, replace=True, n_samples=resample_size, random_state=1)\n",
    "    else:\n",
    "        positive_balanced = positive\n",
    "    if len(negative) != resample_size:\n",
    "        negative_balanced = resample(negative, replace=True, n_samples=resample_size, random_state=1)\n",
    "    else:\n",
    "        negative_balanced = negative\n",
    "\n",
    "    df_continuous_balanced = pd.concat([positive_balanced, negative_balanced])\n",
    "\n",
    "    price_change = df_continuous_balanced['QQQ_'+str(days_predict)]\n",
    "    features = df_continuous_balanced[['VXX', 'VXX_SMA_2', 'VXX_SMA_3', 'QQQ_SMA_50', 'QQQ_SMA_100', 'QQQ_SMA_200', '5yr_rate', '10yr_rate', '30yr_rate', 'RSI', 'RSI_2', 'RSI_3']]\n",
    "\n",
    "    # Split the 'features' and 'income' data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                        price_change,\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 48)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, df_continuous_balanced\n",
    "\n",
    "def models(X_train, X_test, y_train, y_test):\n",
    "    # Train a logistic regression model\n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "    # Train a support vector machine model\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Train a random forest classifier\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(X_train, y_train)\n",
    "\n",
    "    # Train a gradient boosting machine model\n",
    "    gbm = GradientBoostingClassifier()\n",
    "    gbm.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the models on the test set\n",
    "    logistic_regression_accuracy = logistic_regression.score(X_test, y_test)\n",
    "    svm_accuracy = svm.score(X_test, y_test)\n",
    "    random_forest_accuracy = random_forest.score(X_test, y_test)\n",
    "    gbm_accuracy = gbm.score(X_test, y_test)\n",
    "\n",
    "    # Calculate the F-score of each model\n",
    "    logistic_regression_f1_score = f1_score(y_test, logistic_regression.predict(X_test), average='macro')\n",
    "    svm_f1_score = f1_score(y_test, svm.predict(X_test), average='macro')\n",
    "    random_forest_f1_score = f1_score(y_test, random_forest.predict(X_test), average='macro')\n",
    "    gbm_f1_score = f1_score(y_test, gbm.predict(X_test), average='macro')\n",
    "\n",
    "    # Print the accuracy of each model\n",
    "    print('Logistic regression accuracy:', logistic_regression_accuracy.round(4))\n",
    "    print('SVM accuracy:', svm_accuracy.round(4))\n",
    "    print('Random forest accuracy:', random_forest_accuracy.round(4))\n",
    "    print('GBM accuracy:', gbm_accuracy.round(4))\n",
    "\n",
    "    # Print the F-score of each model\n",
    "    print('Logistic regression F-score:', logistic_regression_f1_score.round(4))\n",
    "    print('SVM F-score:', svm_f1_score.round(4))\n",
    "    print('Random forest F-score:', random_forest_f1_score.round(4))\n",
    "    print('GBM F-score:', gbm_f1_score.round(4))\n",
    "\n",
    "    # Print the confusion matrix for the logistic regression model\n",
    "    logistic_regression_confusion_matrix = confusion_matrix(y_test, logistic_regression.predict(X_test))\n",
    "    print('Logistic regression confusion matrix:')\n",
    "    print(logistic_regression_confusion_matrix)\n",
    "\n",
    "    # Print the confusion matrix for the SVM model\n",
    "    svm_confusion_matrix = confusion_matrix(y_test, svm.predict(X_test))\n",
    "    print('SVM confusion matrix:')\n",
    "    print(svm_confusion_matrix)\n",
    "\n",
    "    # Print the confusion matrix for the random forest model\n",
    "    random_forest_confusion_matrix = confusion_matrix(y_test, random_forest.predict(X_test))\n",
    "    print('Random forest confusion matrix:')\n",
    "    print(random_forest_confusion_matrix)\n",
    "\n",
    "    # Print the confusion matrix for the GBM model\n",
    "    gbm_confusion_matrix = confusion_matrix(y_test, gbm.predict(X_test))\n",
    "    print('GBM confusion matrix:')\n",
    "    print(gbm_confusion_matrix)\n",
    "\n",
    "    \"\"\"\n",
    "    [[TP, FP],\n",
    "    [FN, TN]]\n",
    "    \"\"\"\n",
    "\n",
    "def optimize(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Define the hyperparameters to tune for the random forest model\n",
    "    random_forest_hyperparameters = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 3, 5]\n",
    "    }\n",
    "\n",
    "    # Define the hyperparameters to tune for the GBM model\n",
    "    gbm_hyperparameters = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    # Create a grid search object for the random forest model\n",
    "    random_forest_grid_search = GridSearchCV(RandomForestClassifier(), random_forest_hyperparameters, cv=5)\n",
    "\n",
    "    # Create a grid search object for the GBM model\n",
    "    gbm_grid_search = GridSearchCV(GradientBoostingClassifier(), gbm_hyperparameters, cv=5)\n",
    "\n",
    "    # Fit the grid search objects to the training data\n",
    "    random_forest_grid_search.fit(X_train, y_train)\n",
    "    gbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best random forest model\n",
    "    best_random_forest_model = random_forest_grid_search.best_estimator_\n",
    "\n",
    "    # Get the best GBM model\n",
    "    best_gbm_model = gbm_grid_search.best_estimator_\n",
    "\n",
    "    # Predict the classes of the test data using the best random forest model\n",
    "    random_forest_y_pred = best_random_forest_model.predict(X_test)\n",
    "\n",
    "    # Predict the classes of the test data using the best GBM model\n",
    "    gbm_y_pred = best_gbm_model.predict(X_test)\n",
    "\n",
    "    # Calculate the F-score for the random forest model\n",
    "    random_forest_f1_score = f1_score(y_test, random_forest_y_pred, average='macro')\n",
    "\n",
    "    # Calculate the confusion matrix for the random forest model\n",
    "    random_forest_confusion_matrix = confusion_matrix(y_test, random_forest_y_pred)\n",
    "\n",
    "    # Calculate the F-score for the GBM model\n",
    "    gbm_f1_score = f1_score(y_test, gbm_y_pred, average='macro')\n",
    "\n",
    "    # Calculate the confusion matrix for the GBM model\n",
    "    gbm_confusion_matrix = confusion_matrix(y_test, gbm_y_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print('Random forest F-score:', random_forest_f1_score)\n",
    "    print('Random forest confusion matrix:')\n",
    "    print(random_forest_confusion_matrix)\n",
    "\n",
    "    print('GBM F-score:', gbm_f1_score)\n",
    "    print('GBM confusion matrix:')\n",
    "    print(gbm_confusion_matrix)\n",
    "\n",
    "    return best_random_forest_model, best_gbm_model\n",
    "\n",
    "def savemodels(best_random_forest_model, best_gbm_model):\n",
    "    import pickle\n",
    "\n",
    "    # Save the best random forest model\n",
    "    with open('random_forest_model2.pkl', 'wb') as file_object:\n",
    "        pickle.dump(best_random_forest_model, file_object)\n",
    "\n",
    "    # Save the best GBM model\n",
    "    with open('gbm_model2.pkl', 'wb') as file_object:\n",
    "        pickle.dump(best_gbm_model, file_object)\n",
    "\n",
    "    print('Models Saved')\n",
    "\n",
    "def predict(model, df_continuous, days_predict):\n",
    "\n",
    "    model = model\n",
    "\n",
    "    with open(str(model)+'2.pkl', 'rb') as file_object:\n",
    "        best_model = pickle.load(file_object)\n",
    "\n",
    "    df_predicts = df_continuous[['VXX', 'VXX_SMA_2', 'VXX_SMA_3', 'QQQ_SMA_50', 'QQQ_SMA_100', 'QQQ_SMA_200', '5yr_rate', '10yr_rate', '30yr_rate', 'RSI', 'RSI_2', 'RSI_3']]\n",
    "\n",
    "    # Assuming you have a dataframe named 'df_predicts'\n",
    "    new_data_predictions = best_model.predict(df_predicts)\n",
    "    new_data_probabilities = best_model.predict_proba(df_predicts)\n",
    "\n",
    "    # Create a copy of the original dataframe 'df_discrete'\n",
    "    predictions_df = df_continuous[['date', 'QQQ']]\n",
    "    predictions_df = pd.DataFrame(predictions_df)\n",
    "\n",
    "    # Add columns for predictions and probability estimates\n",
    "    predictions_df['Predictions'] = new_data_predictions\n",
    "    predictions_df['Probability_'+str(days_predict)+'_Day_Decline'] = new_data_probabilities[:, 0]\n",
    "    predictions_df['Probability_'+str(days_predict)+'_Day_Rise'] = new_data_probabilities[:, 1]\n",
    "\n",
    "    # Round the probabilities to the nearest thousandth\n",
    "    predictions_df['Probability_'+str(days_predict)+'_Day_Decline'] = predictions_df['Probability_'+str(days_predict)+'_Day_Decline'].round(4)\n",
    "    predictions_df['Probability_'+str(days_predict)+'_Day_Rise'] = predictions_df['Probability_'+str(days_predict)+'_Day_Rise'].round(4)\n",
    "    \n",
    "    '''\n",
    "    idx_pred = days_predict - 1\n",
    "\n",
    "    # Create a new column called \"Prediction_n_Days_From_Now\"\n",
    "    predictions_df[\"Prediction_\"+str(days_predict)+\"_Days_Ago\"] = predictions_df[\"Predictions\"].shift(-idx_pred)\n",
    "    predictions_df[\"QQQ_\"+str(days_predict)+\"_Days_Ago\"] = predictions_df[\"QQQ\"].shift(-idx_pred)\n",
    "\n",
    "    # Create a new column called \"QQQ_n_Days_Ago_Greater_Than_QQQ\"\n",
    "    predictions_df[\"Actual\"] = predictions_df[\"QQQ_\"+str(days_predict)+\"_Days_Ago\"] < predictions_df[\"QQQ\"]\n",
    "    # Convert the new column to integers\n",
    "    predictions_df[\"Actual\"] = predictions_df[\"Actual\"].astype(int)\n",
    "\n",
    "    # Create a new column called \"QQQ_n_Days_Ago_Greater_Than_QQQ\"\n",
    "    predictions_df[\"Accuracy\"] = predictions_df[\"Prediction_\"+str(days_predict)+\"_Days_Ago\"] == predictions_df[\"Actual\"]\n",
    "    # Convert the new column to integers\n",
    "    predictions_df[\"Accuracy\"] = predictions_df[\"Accuracy\"].astype(int)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    predictions = predictions_df\n",
    "\n",
    "    predictions.to_csv(str(model)+'preds2.csv', index=False)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>QQQ</th>\n",
       "      <th>VXX</th>\n",
       "      <th>QQQ_SMA_50</th>\n",
       "      <th>QQQ_SMA_100</th>\n",
       "      <th>QQQ_SMA_200</th>\n",
       "      <th>VXX_SMA_2</th>\n",
       "      <th>VXX_SMA_3</th>\n",
       "      <th>5yr_rate</th>\n",
       "      <th>10yr_rate</th>\n",
       "      <th>30yr_rate</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_2</th>\n",
       "      <th>RSI_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>370.0700</td>\n",
       "      <td>20.8000</td>\n",
       "      <td>364.2944</td>\n",
       "      <td>367.5613</td>\n",
       "      <td>342.93875</td>\n",
       "      <td>20.400</td>\n",
       "      <td>20.393333</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.77</td>\n",
       "      <td>69.8027</td>\n",
       "      <td>69.50350</td>\n",
       "      <td>68.468267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>372.9400</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>364.4302</td>\n",
       "      <td>367.5296</td>\n",
       "      <td>342.52705</td>\n",
       "      <td>20.190</td>\n",
       "      <td>20.286667</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.49</td>\n",
       "      <td>4.64</td>\n",
       "      <td>69.2043</td>\n",
       "      <td>67.80105</td>\n",
       "      <td>66.719467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>372.7000</td>\n",
       "      <td>20.3800</td>\n",
       "      <td>364.4668</td>\n",
       "      <td>367.4795</td>\n",
       "      <td>342.10420</td>\n",
       "      <td>20.430</td>\n",
       "      <td>20.680000</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.75</td>\n",
       "      <td>66.3978</td>\n",
       "      <td>65.47705</td>\n",
       "      <td>63.804233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>369.2100</td>\n",
       "      <td>20.4800</td>\n",
       "      <td>364.3480</td>\n",
       "      <td>367.4551</td>\n",
       "      <td>341.68550</td>\n",
       "      <td>20.830</td>\n",
       "      <td>21.116667</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.84</td>\n",
       "      <td>64.5563</td>\n",
       "      <td>62.50745</td>\n",
       "      <td>60.531933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>367.7100</td>\n",
       "      <td>21.1800</td>\n",
       "      <td>364.2442</td>\n",
       "      <td>367.4220</td>\n",
       "      <td>341.25285</td>\n",
       "      <td>21.435</td>\n",
       "      <td>21.776667</td>\n",
       "      <td>4.49</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.77</td>\n",
       "      <td>60.4586</td>\n",
       "      <td>58.51975</td>\n",
       "      <td>52.520433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       QQQ      VXX  QQQ_SMA_50  QQQ_SMA_100  QQQ_SMA_200  \\\n",
       "0 2023-11-09  370.0700  20.8000    364.2944     367.5613    342.93875   \n",
       "1 2023-11-08  372.9400  20.0000    364.4302     367.5296    342.52705   \n",
       "2 2023-11-07  372.7000  20.3800    364.4668     367.4795    342.10420   \n",
       "3 2023-11-06  369.2100  20.4800    364.3480     367.4551    341.68550   \n",
       "4 2023-11-03  367.7100  21.1800    364.2442     367.4220    341.25285   \n",
       "\n",
       "   VXX_SMA_2  VXX_SMA_3 5yr_rate 10yr_rate 30yr_rate      RSI     RSI_2  \\\n",
       "0     20.400  20.393333     4.65      4.63      4.77  69.8027  69.50350   \n",
       "1     20.190  20.286667     4.51      4.49      4.64  69.2043  67.80105   \n",
       "2     20.430  20.680000     4.53      4.58      4.75  66.3978  65.47705   \n",
       "3     20.830  21.116667     4.60      4.67      4.84  64.5563  62.50745   \n",
       "4     21.435  21.776667     4.49      4.57      4.77  60.4586  58.51975   \n",
       "\n",
       "       RSI_3  \n",
       "0  68.468267  \n",
       "1  66.719467  \n",
       "2  63.804233  \n",
       "3  60.531933  \n",
       "4  52.520433  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_continuous = preprocess()\n",
    "df_continuous.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative(df_continuous):\n",
    "    df_continuous2 = df_continuous.copy()\n",
    "    df_continuous2 = pd.DataFrame(df_continuous2)\n",
    "\n",
    "    # Convert the specified columns to numeric\n",
    "    columns_to_convert = [\n",
    "        \"QQQ\",\n",
    "        'VXX',\n",
    "        \"QQQ_SMA_50\",\n",
    "        \"QQQ_SMA_100\",\n",
    "        \"QQQ_SMA_200\",\n",
    "        \"VXX_SMA_2\",\n",
    "        \"VXX_SMA_3\",\n",
    "        \"5yr_rate\",\n",
    "        \"10yr_rate\",\n",
    "        \"30yr_rate\",\n",
    "        \"RSI\",\n",
    "        \"RSI_2\",\n",
    "        \"RSI_3\"\n",
    "    ]\n",
    "\n",
    "    for col in columns_to_convert:\n",
    "        df_continuous2[col] = pd.to_numeric(df_continuous2[col], errors='coerce')\n",
    "\n",
    "    # Change to relative data\n",
    "    df_continuous2 = df_continuous2.sort_index(ascending=False)\n",
    "\n",
    "    # Calculate QQQSMA50/QQQ\n",
    "    df_continuous2['QQQ_SMA_50'] = df_continuous2['QQQ_SMA_50'] / df_continuous2['QQQ']\n",
    "\n",
    "    # Calculate QQQSMA100/QQQ\n",
    "    df_continuous2['QQQ_SMA_100'] = df_continuous2['QQQ_SMA_100'] / df_continuous2['QQQ']\n",
    "\n",
    "    # Calculate QQQSMA200/QQQ\n",
    "    df_continuous2['QQQ_SMA_200'] = df_continuous2['QQQ_SMA_200'] / df_continuous2['QQQ']\n",
    "\n",
    "    # Calculate VXX_SMA_2/VXX\n",
    "    df_continuous2['VXX_SMA_2'] = df_continuous2['VXX_SMA_2'] / df_continuous2['VXX']\n",
    "\n",
    "    # Calculate VXX_SMA_3/VXX\n",
    "    df_continuous2['VXX_SMA_3'] = df_continuous2['VXX_SMA_3'] / df_continuous2['VXX']\n",
    "\n",
    "    # Calculate the current value/the trailing 7 day average for 5yr_rate\n",
    "    df_continuous2['5yr_rate_ave'] = df_continuous2['5yr_rate'].rolling(window=7).mean()\n",
    "    df_continuous2['5yr_rate'] = df_continuous2['5yr_rate'] / df_continuous2['5yr_rate_ave']\n",
    "    df_continuous2 = df_continuous2.drop('5yr_rate_ave', axis=1)\n",
    "\n",
    "    # Calculate the current value/the trailing 7 day average for 10yr_rate\n",
    "    df_continuous2['10yr_rate_ave'] = df_continuous2['10yr_rate'].rolling(window=7).mean()\n",
    "    df_continuous2['10yr_rate'] = df_continuous2['10yr_rate'] / df_continuous2['10yr_rate_ave']\n",
    "    df_continuous2 = df_continuous2.drop('10yr_rate_ave', axis=1)\n",
    "\n",
    "    # Calculate the current value/the trailing 7 day average for 30yr_rate\n",
    "    df_continuous2['30yr_rate_ave'] = df_continuous2['30yr_rate'].rolling(window=7).mean()\n",
    "    df_continuous2['30yr_rate'] = df_continuous2['30yr_rate'] / df_continuous2['30yr_rate_ave']\n",
    "    df_continuous2 = df_continuous2.drop('30yr_rate_ave', axis=1)\n",
    "\n",
    "    # Calculate RSI_2/RSI\n",
    "    df_continuous2['RSI_2'] = df_continuous2['RSI_2'] / df_continuous2['RSI']\n",
    "\n",
    "    # Calculate RSI_3/RSI\n",
    "    df_continuous2['RSI_3'] = df_continuous2['RSI_3'] / df_continuous2['RSI']\n",
    "\n",
    "    df_continuous2 = df_continuous2.sort_index(ascending=True)\n",
    "    df_continuous2 = df_continuous2.dropna()\n",
    "\n",
    "    return df_continuous2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.8926\n",
      "SVM accuracy: 0.9027\n",
      "Random forest accuracy: 0.9463\n",
      "GBM accuracy: 0.9262\n",
      "Logistic regression F-score: 0.8926\n",
      "SVM F-score: 0.9026\n",
      "Random forest F-score: 0.9463\n",
      "GBM F-score: 0.9261\n",
      "Logistic regression confusion matrix:\n",
      "[[398  42]\n",
      " [ 54 400]]\n",
      "SVM confusion matrix:\n",
      "[[413  27]\n",
      " [ 60 394]]\n",
      "Random forest confusion matrix:\n",
      "[[428  12]\n",
      " [ 36 418]]\n",
      "GBM confusion matrix:\n",
      "[[424  16]\n",
      " [ 50 404]]\n"
     ]
    }
   ],
   "source": [
    "df_continuous = relative(df_continuous)\n",
    "\n",
    "days_predict = 22\n",
    "\n",
    "X_train, X_test, y_train, y_test, df_continuous_scaled = traintest(days_predict, df_continuous)\n",
    "\n",
    "models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>QQQ</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Probability_22_Day_Decline</th>\n",
       "      <th>Probability_22_Day_Rise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>370.07</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3505</td>\n",
       "      <td>0.6495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>372.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.9604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>372.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>369.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>367.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.8681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>363.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2499</td>\n",
       "      <td>0.7501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>356.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.5989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>350.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4237</td>\n",
       "      <td>0.5763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-10-30</td>\n",
       "      <td>349.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>0.5699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-10-26</td>\n",
       "      <td>343.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>0.0633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     QQQ  Predictions  Probability_22_Day_Decline  \\\n",
       "0  2023-11-09  370.07            1                      0.3505   \n",
       "1  2023-11-08  372.94            1                      0.0396   \n",
       "2  2023-11-07  372.70            1                      0.1175   \n",
       "3  2023-11-06  369.21            1                      0.0268   \n",
       "4  2023-11-03  367.71            1                      0.1319   \n",
       "5  2023-11-02  363.44            1                      0.2499   \n",
       "6  2023-11-01  356.96            1                      0.4011   \n",
       "7  2023-10-31  350.87            1                      0.4237   \n",
       "8  2023-10-30  349.20            1                      0.4301   \n",
       "10 2023-10-26  343.66            0                      0.9367   \n",
       "\n",
       "    Probability_22_Day_Rise  \n",
       "0                    0.6495  \n",
       "1                    0.9604  \n",
       "2                    0.8825  \n",
       "3                    0.9732  \n",
       "4                    0.8681  \n",
       "5                    0.7501  \n",
       "6                    0.5989  \n",
       "7                    0.5763  \n",
       "8                    0.5699  \n",
       "10                   0.0633  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the original dataframe 'df_discrete'\n",
    "predictions_df = pd.merge(df_continuous['date'], df_continuous_scaled, left_index=True, right_index=True)\n",
    "\n",
    "# Drop the scaled QQQ column from the predictions_df DataFrame\n",
    "predictions_df = predictions_df.drop('QQQ', axis=1)\n",
    "\n",
    "# Add historic close price to output\n",
    "predictions_df['QQQ'] = df_continuous['QQQ']\n",
    "predictions_df = predictions_df.drop_duplicates()\n",
    "#'gbm_model'\n",
    "#'random_forest_model'\n",
    "model = 'random_forest_model'\n",
    "\n",
    "predictions = predict(model, predictions_df, days_predict)\n",
    "\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest F-score: 0.874498716689173\n",
      "Random forest confusion matrix:\n",
      "[[398  42]\n",
      " [ 69 376]]\n",
      "GBM F-score: 0.9265442936951316\n",
      "GBM confusion matrix:\n",
      "[[415  25]\n",
      " [ 40 405]]\n"
     ]
    }
   ],
   "source": [
    "best_random_forest_model, best_gbm_model = optimize(X_train, X_test, y_train, y_test)\n",
    "\n",
    "savemodels(best_random_forest_model, best_gbm_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
